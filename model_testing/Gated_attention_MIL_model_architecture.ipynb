{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJtgf4KYplpR"
      },
      "outputs": [],
      "source": [
        "class AttentionMILModel(nn.Module):\n",
        "    def __init__(self, num_classes, backbone_name='resnet18', pretrained=True,\n",
        "                 feature_dim=512, attention_dim=128, attention_heads=1, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.L = feature_dim\n",
        "        self.D = attention_dim\n",
        "        self.K = attention_heads\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # backbone\n",
        "        self.feature_extractor, backbone_out_dim = self._get_backbone(backbone_name, pretrained)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.feature_embedder = nn.Sequential(\n",
        "            nn.Linear(backbone_out_dim, self.L),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Gated attention mechanism\n",
        "        self.attention_V = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.attention_U = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.attention_weights = nn.Linear(self.D, self.K)\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Linear(self.L * self.K, self.num_classes)\n",
        "\n",
        "    def _get_backbone(self, name, pretrained):\n",
        "        weights = 'DEFAULT' if pretrained else None\n",
        "\n",
        "        if name == \"resnet18\":\n",
        "            model = resnet18(weights=weights)\n",
        "            return nn.Sequential(*list(model.children())[:-2]), 512\n",
        "        elif name == \"resnet50\":\n",
        "            model = resnet50(weights=weights)\n",
        "            return nn.Sequential(*list(model.children())[:-2]), 2048\n",
        "        elif name == \"resnet152\":\n",
        "            model = resnet152(weights=weights)\n",
        "            return nn.Sequential(*list(model.children())[:-2]), 2048\n",
        "        elif name == \"efficientnet_b0\":\n",
        "            model = efficientnet_b0(weights=weights)\n",
        "            return model.features, 1280\n",
        "        elif name == \"vgg16\":\n",
        "            model = vgg16(weights=weights)\n",
        "            return nn.Sequential(*list(model.features.children())[:-1]), 512\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {name}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C, H, W = x.shape\n",
        "        x = x.view(B * N, C, H, W)\n",
        "\n",
        "        features = self.feature_extractor(x)\n",
        "        features = self.pool(features).view(B * N, -1)\n",
        "        embedded = self.feature_embedder(features)\n",
        "\n",
        "        # Gated attention\n",
        "        A_v = self.attention_V(embedded)\n",
        "        A_u = self.attention_U(embedded)\n",
        "        A = self.attention_weights(A_v * A_u)\n",
        "        A = A.view(B, N, self.K)\n",
        "        A = F.softmax(A, dim=1)\n",
        "\n",
        "        A = A.transpose(1, 2)  # (B, K, N)\n",
        "        embedded = embedded.view(B, N, self.L)\n",
        "        M = torch.bmm(A, embedded)  # (B, K, L)\n",
        "\n",
        "        # Classification\n",
        "        M = M.view(B, -1)  # Flatten\n",
        "        logits = self.classifier(M)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        return logits, probs, A.transpose(1, 2)\n",
        "\n",
        "    def calculate_objective(self, x_batch, y_batch, criterion):\n",
        "        logits, _, _ = self.forward(x_batch)\n",
        "        return criterion(logits, y_batch)\n",
        "\n",
        "    def calculate_prediction_accuracy(self, x_batch, y_batch):\n",
        "        logits, _, _ = self.forward(x_batch)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct = preds.eq(y_batch).sum().item()\n",
        "        total = y_batch.size(0)\n",
        "        return preds, correct / total, correct, total\n",
        "\n",
        "    def get_attention_map(self, bag_tensor):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            if bag_tensor.ndim != 4:\n",
        "              raise ValueError(f\"Expected input of shape (bag_size, 3, H, W), got {bag_tensor.shape}\")\n",
        "\n",
        "            N, C, H, W = bag_tensor.shape  # bag_size, channels, height, width\n",
        "\n",
        "            # Move input to device\n",
        "            device = next(self.parameters()).device\n",
        "            bag_tensor = bag_tensor.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            features = self.feature_extractor(bag_tensor)\n",
        "            features = self.pool(features).view(N, -1)\n",
        "            embedded = self.feature_embedder(features)\n",
        "\n",
        "            # gated attention\n",
        "            A_v = self.attention_V(embedded)\n",
        "            A_u = self.attention_U(embedded)\n",
        "            A_scores = self.attention_weights(A_v * A_u)\n",
        "\n",
        "            A_weights = F.softmax(A_scores, dim=0)\n",
        "            attention = A_weights[:, 0].cpu().numpy()\n",
        "\n",
        "            return attention"
      ]
    }
  ]
}